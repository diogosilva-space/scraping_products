# Sistema de Scraping de Produtos

Sistema automatizado para extraÃ§Ã£o de dados de produtos de sites de e-commerce usando Puppeteer.

## ğŸš€ Funcionalidades

- Scraping automÃ¡tico de produtos com rolagem infinita
- Suporte a mÃºltiplos sites com configuraÃ§Ãµes personalizadas
- ExtraÃ§Ã£o de dados estruturados (referÃªncia, nome, descriÃ§Ã£o, cores, imagens, etc.)
- Sistema de configuraÃ§Ã£o modular por site
- Suporte a login quando necessÃ¡rio
- Salvamento de dados em formato JSON
- **ğŸ†• SincronizaÃ§Ã£o automÃ¡tica com API da djob.com.br**
- **ğŸ†• Envio em lote de produtos para a nuvem**
- **ğŸ†• Sistema de retry e tratamento de erros robusto**
- Interface de linha de comando com progresso visual

## ğŸ“‹ Sites Suportados

- [Spot Gifts](https://www.spotgifts.com.br/pt/catalogo/) - CatÃ¡logo de brindes corporativos
- [XBZ Brindes](https://www.xbzbrindes.com.br/) - Brindes personalizados

## ğŸ› ï¸ InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone <seu-repositorio>
cd scraping-products

# Instale as dependÃªncias
npm install

# Instale o Puppeteer (pode demorar na primeira vez)
npx puppeteer browsers install chrome
```

### ğŸ”„ Atualizando DependÃªncias

Se vocÃª encontrar avisos de versÃµes desatualizadas:

```bash
# Execute o script de atualizaÃ§Ã£o automÃ¡tica
chmod +x update-dependencies.sh
./update-dependencies.sh

# Ou atualize manualmente
npm update
npx puppeteer browsers install chrome
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# ConfiguraÃ§Ãµes gerais
HEADLESS=true
DELAY_BETWEEN_REQUESTS=1000
MAX_RETRIES=3

# Credenciais de login (quando necessÃ¡rio)
SPOTGIFTS_EMAIL=seu-email@exemplo.com
SPOTGIFTS_PASSWORD=sua-senha
XBZBRINDES_EMAIL=seu-email@exemplo.com
XBZBRINDES_PASSWORD=sua-senha

# ========================================
# CONFIGURAÃ‡Ã•ES DA API DJOB.COM.BR
# ========================================

# Credenciais de acesso Ã  API
DJOB_USERNAME=seu_email@exemplo.com
DJOB_PASSWORD=sua_senha_aqui
DJOB_API_KEY=sua_chave_api_aqui

# SincronizaÃ§Ã£o automÃ¡tica
DJOB_AUTO_SYNC=true
DJOB_SYNC_AFTER_SCRAPING=true
DJOB_BATCH_SIZE=10
DJOB_BATCH_DELAY=2000
```

### 2. ConfiguraÃ§Ã£o dos Sites

Cada site tem seu arquivo de configuraÃ§Ã£o em `src/config/` com:
- URLs base
- Seletores CSS/XPath
- ConfiguraÃ§Ãµes de rolagem
- Mapeamento de campos

## ğŸš€ Uso

### Scraping de um site especÃ­fico:

```bash
# Scraping do Spot Gifts
npm run scrape:spotgifts

# Scraping do XBZ Brindes
npm run scrape:xbzbrindes
```

### Scraping de todos os sites:

```bash
npm start
```

### ğŸ†• SincronizaÃ§Ã£o com a API:

```bash
# Sincroniza todos os produtos existentes
npm run sync

# Sincroniza produtos de um site especÃ­fico
npm run sync:spotgifts
npm run sync:xbzbrindes

# Exibe estatÃ­sticas de sincronizaÃ§Ã£o
npm run stats
```

### ğŸ†• Comandos de linha de comando:

```bash
# SincronizaÃ§Ã£o direta
node src/index.js --sync
node src/index.js --sync:site "Spot Gifts"
node src/index.js --stats

# CombinaÃ§Ã£o de scraping + sincronizaÃ§Ã£o
node src/index.js --site "Spot Gifts"  # Scraping + sincronizaÃ§Ã£o automÃ¡tica
```

## ğŸ“Š Estrutura dos Dados

Cada produto extraÃ­do contÃ©m:

```json
{
  "referencia": "string (obrigatÃ³rio)",
  "nome": "string (obrigatÃ³rio)",
  "descricao": "string (obrigatÃ³rio)",
  "cores": ["array de strings (obrigatÃ³rio)"],
  "imagens": ["array de URLs (obrigatÃ³rio)"],
  "categorias": ["array de strings (opcional)"],
  "informacoes_adicionais": "string (opcional)",
  "preco": "number (opcional)",
  "url_produto": "string",
  "data_extracao": "ISO string"
}
```

## ğŸ“ Estrutura do Projeto

```
src/
â”œâ”€â”€ config/           # ConfiguraÃ§Ãµes dos sites
â”‚   â””â”€â”€ api.js        # ğŸ†• ConfiguraÃ§Ãµes da API
â”œâ”€â”€ scrapers/         # ImplementaÃ§Ãµes dos scrapers
â”œâ”€â”€ utils/            # UtilitÃ¡rios compartilhados
â”‚   â”œâ”€â”€ apiClient.js  # ğŸ†• Cliente da API
â”‚   â””â”€â”€ syncManager.js # ğŸ†• Gerenciador de sincronizaÃ§Ã£o
â”œâ”€â”€ models/           # Modelos de dados
â””â”€â”€ index.js          # Arquivo principal
```

## ğŸ”§ PersonalizaÃ§Ã£o

Para adicionar um novo site:

1. Crie um arquivo de configuraÃ§Ã£o em `src/config/`
2. Implemente o scraper em `src/scrapers/`
3. Adicione o script no `package.json`
4. Configure as variÃ¡veis de ambiente se necessÃ¡rio

## âš ï¸ ConsideraÃ§Ãµes Legais

- Respeite os termos de uso dos sites
- Use delays apropriados entre requisiÃ§Ãµes
- NÃ£o sobrecarregue os servidores
- Considere usar APIs oficiais quando disponÃ­veis

## ğŸ†• IntegraÃ§Ã£o com API

O sistema agora integra automaticamente com a API da [djob.com.br](https://api.djob.com.br/wp-json/api/v1/documentacao):

- **SincronizaÃ§Ã£o automÃ¡tica**: Produtos sÃ£o enviados para a nuvem apÃ³s o scraping
- **Envio em lote**: Processamento eficiente de mÃºltiplos produtos
- **Tratamento de erros**: Sistema robusto de retry e fallback
- **ValidaÃ§Ã£o**: Produtos sÃ£o validados antes do envio
- **Logs detalhados**: Acompanhamento completo do processo de sincronizaÃ§Ã£o

## ğŸ“ Logs

O sistema gera logs detalhados em:
- Console com cores
- Arquivo `logs/scraping.log`
- RelatÃ³rios de progresso em tempo real

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

MIT License - veja o arquivo LICENSE para detalhes.
